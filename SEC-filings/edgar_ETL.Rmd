---
title: "edgar"
author: "Gibran Makyanie"
date: "03/04/2020"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_knit$set(root.dir = '/Volumes/Buku Gibran/edgar')
knitr::opts_chunk$set(eval = FALSE)
library(tidyverse)
library(edgar)
library(XML)
library(lubridate)
library(tm)
library(RSQLite)
library(tidytext)
library(udpipe)
library(rvest)
library(readxl)
library(qdap)
library(sentimentr)
library(textfeatures)
library(BatchGetSymbols)
library(lubridate)
library(DataExplorer)
library(gridExtra)
library(stm)
```

# ETL Initiation

## Download Financial Reports
```{r}
# ----- The S&P 500
sp500 <- read_csv('sp500.csv')

# ----- Download MDA chapter 10-K and HTML of 10-Q
edgar::getMgmtDisc(cik.no = sp500$CIK, filing.year = c(2009:2019))
edgar::getFilingsHTML(cik.no = sp500$CIK, form.type = '10-Q', filing.year = c(2009:2019))
```

## Setup SQLite database
```{r}
# ----- Initiate SQLite database
conn <- dbConnect(RSQLite::SQLite(), "edgar.db")
dbWriteTable(conn,"sp500", sp500) # create sp500 table for information about S&P500
rm(sp500)
```

```{r}
# ----- Insert Master Indexes to db
master_index_list <-list.files('Master Indexes')
for(i in 1:length(master_index_list)) {

  load(paste0('Master Indexes/',master_index_list[i]))
  
  local_df <- year.master %>%
    filter(cik %in% sp500$CIK, form.type %in% c('10-Q', '10-K')) %>%
    mutate(date.filed = as.Date(date.filed)) %>%
    mutate(year_filed = year(date.filed)) %>%
    mutate(accession.number = gsub(".*/", "", edgar.link)) %>%
    mutate(accession.number = gsub('.txt','',accession.number)) %>%
    select(-edgar.link)
  
  colnames(local_df) <- gsub("\\.", "_", colnames(local_df)) # column names with dots (.) will confuse SQL

  dbWriteTable(conn,"master_index", local_df, append = TRUE) # create master_index Table
}

rm(local_df, year.master)
```

```{r}
# ----- Check Records
dbGetQuery(conn, 'SELECT count(cik) from master_index') # 20678
dbGetQuery(conn, 'SELECT count(cik) from sp500') # 505

dbDisconnect(conn)
```

## Import Stop Words from Loughran & McDonald's
```{r}
# ----- Stopwords by Loughran McDonald
stopw_loughran_mcdonald <- c()
stopw_dictionaries <-list.files('stopw_loughran_mcdonald')

for (i in 1:length(stopw_dictionaries)) {
    file_path <-paste('stopw_loughran_mcdonald', stopw_dictionaries[i],sep="/")
    local_list <- read_lines(file_path)
    local_list <- iconv(local_list, "ASCII", "UTF-8", sub="") %>% tolower()
    stopw_loughran_mcdonald <- c(stopw_loughran_mcdonald, local_list)
}

# ----- Add customised stopwords
stopw_custom <- c('vs', 'financial', 'statement', 'exhibit','report','figure','fig','tab','table', 'mda', 'company', 'footnote', 'page')


# ----- Finalising Stopwords
stopw_final <- c(stopw_loughran_mcdonald, stopw_custom)
rm(stopw_loughran_mcdonald, stopw_custom, stopw_dictionaries, file_path, local_list)

# ----- Get the udpipe model
ud_model <- udpipe_download_model(language = "english", overwrite = F)
ud_model <- udpipe_load_model(ud_model$file_model)
```

# ETL Process

## Process MD&A chapter 10K reports
```{r}
conn <- dbConnect(RSQLite::SQLite(), "edgar.db")

the_index <- dbGetQuery(conn, 'SELECT accession_number FROM master_index WHERE form_type = "10-K" ORDER BY date_filed')

split_size <- 50
split_the_index <- split(the_index$accession_number, ceiling(seq_along(the_index$accession_number)/split_size))

rm(the_index)

for (s in 1:length(split_the_index)) {
file_pattern <- paste0(split_the_index[[s]], '.txt')
listed_files <- list.files('MD&A section text', pattern = paste0(file_pattern, collapse = "|"))
file_path <- paste0('MD&A section text/', listed_files)

rm(file_pattern, listed_files)

for(i in 1:length(file_path)) {
  
  # ----- Clean Text
  text_file <- read_lines(file_path[i])
  text_transformed <- tibble(
               company_name = tolower(gsub('Company Name: ','',text_file[2])), 
               accession_number = gsub('Accession Number: ','',text_file[5]), 
               mgmtdisc = gsub(" s "," ",tolower(text_file[8]) %>% 
                                 removePunctuation()) %>%
                                 removeNumbers() %>%
                                 stripWhitespace())
  
  rm(text_file)
  
  company_name <- unlist(str_split(text_transformed$company_name, " ", n= nchar(text_transformed$company_name)))[1]
  
  sub_this <- c("item","management", "managements", "discussion and analysis", "financial condition", "results of operations",
                company_name)
  text_transformed$cleaned <- gsub(paste0(sub_this, collapse = '|'),"", text_transformed$mgmtdisc)
  
  rm(company_name, sub_this)
  
  # ----- Tokenisation and Part-of-speech Tagging
  tokenised <- text_transformed %>%
    select(accession_number, cleaned) %>%
    unnest_tokens(word, cleaned) %>%
    group_by(accession_number, word) %>%
    filter(!word %in% stopw_final)
  
    rm(text_transformed)
  
      # Udpipe Annotating
      local_df <- udpipe_annotate(tokenised$word,
                        doc_id = tokenised$accession_number,
                        object = ud_model) %>% as.data.frame()
      rm(tokenised)
  
      # Get nouns only
      annotated_nouns <- local_df %>% 
        filter(upos == "NOUN") %>%
        select(doc_id,lemma) %>% 
        group_by(doc_id) %>% 
        summarise(cleaned_noun = paste(lemma, collapse = " ")) %>% 
        rename(accession_number = doc_id)
      
      # Get the most important POS
      annotated_full <- local_df %>% 
        filter(upos %in% c("ADV","ADJ","NOUN", "AUX", "PART")) %>%
        select(doc_id,lemma) %>% 
        group_by(doc_id) %>% 
        summarise(cleaned_text = paste(lemma, collapse = " ")) %>% 
        rename(accession_number = doc_id)
      
      # Store the data into lists we created before for loop
      local_df <- annotated_nouns %>%
      left_join(annotated_full, by= 'accession_number')
      
  # ----- Insertion to SQL table
  dbWriteTable(conn,"cleaned_10k_mda", local_df, append = TRUE) # create master_index Table
  
  temp_report <- dbGetQuery(conn, paste0('SELECT cik, company_name, year_filed, form_type, cleaned_10k_mda.accession_number 
                         FROM master_index LEFT JOIN cleaned_10k_mda ON cleaned_10k_mda.accession_number = master_index.accession_number 
                         WHERE cleaned_10k_mda.accession_number = "',local_df$accession_number[1],'"'))
  
  print(paste(temp_report$form_type, temp_report$year_filed, 'report for CIK:',temp_report$cik, temp_report$company_name, 'has been processed.' ))
      
  rm(annotated_nouns, annotated_full, local_df, temp_report)
  }
}
rm(file_path)
```


## Process MD&A chapter 10Q reports
```{r}

the_index <- dbGetQuery(conn, 'SELECT cik, accession_number FROM master_index WHERE form_type = "10-Q" ')
z = 0
split_the_index <- split(the_index, with(the_index, interaction(cik)), drop = TRUE)
rm(the_index)


for (s in 1:length(split_the_index)) {
file_pattern <- paste0(split_the_index[[s]]$accession_number, '.html')
listed_files <- list.files(paste0('Edgar filings_HTML view/Form 10-Q/',split_the_index[[s]]$cik[1]), pattern = paste0(file_pattern, collapse = "|"))
accession_number <- split_the_index[[s]]$accession_number
file_path <- paste0('Edgar filings_HTML view/Form 10-Q/',split_the_index[[s]]$cik[1],'/', listed_files)

rm(file_pattern, listed_files)

  for(i in 1:length(file_path)) {
  doc <- read_html(file_path[i],  options = "HUGE") %>%
    html_text() %>%
    tolower() %>%
    removePunctuation() %>%
    removeNumbers %>%
    stripWhitespace()
  
  doc_begin <- regmatches(doc,gregexpr("(?<=notes to).*",doc,perl=TRUE))[[1]] # regex to slice document from chapter notes on financial statements
  if(length(doc_begin) > 0) {doc_begin <- doc_begin} else {doc_begin <- regmatches(doc,gregexpr("(?<=notes).*",doc,perl=TRUE))[[1]]} 
  if(length(doc_begin) > 0) {doc_begin <- doc_begin} else {doc_begin <- regmatches(doc,gregexpr("(?<=part).*",doc,perl=TRUE))[[1]]} 
  doc <- regmatches(doc_begin,gregexpr(".*(?<=item exhibits)",doc,perl=TRUE))[[1]][1] # regex to slice document until chapter "exhibits"  
  rm(doc_begin)
  
  sub_this <- c("table of contents",
                "page",
                "item",
                "part",
                "financial information",
                "financial statements",
                "summary of significant accounting policies",
                "(continued)",
                "financial intstruments",
                "derivatives and hedging activities",
                "fair value hedges",
                "fair value measurements",
                "results of operations",
                "financial statements",
                "consolidated statements",
                "consolidated financial",
                "notes to consolidated financial statements",
                "quantitative and qualitative disclosure about market risk",
                "controls and procedures",
                "other information",
                "legal proceedings",
                "risk factors",
                "unregistered sales of equity securities and use of proceeds",
                "management discussion and analysis of financial condition and results of operations",
                "managements discussion and analysis of financial condition and results of operations",
                "exhibits") #company_name
  doc <- gsub(paste0(sub_this, collapse = '|')," ", doc)
  
  text_transformed <- tibble(accession_number = accession_number[i], cleaned = doc)
  rm(doc)
  
  # ----- Tokenisation and Part-of-speech Tagging
    tokenised <- text_transformed %>%
      select(accession_number, cleaned) %>%
      unnest_tokens(word, cleaned) %>%
      group_by(accession_number, word) %>%
      filter(!word %in% stopw_final)
    
      rm(text_transformed)
    
        # Udpipe Annotating
        local_df <- udpipe_annotate(tokenised$word,
                          doc_id = tokenised$accession_number,
                          object = ud_model) %>% as.data.frame()
        rm(tokenised)
    
        # Get nouns only
        annotated_nouns <- local_df %>% 
          filter(upos == "NOUN") %>%
          select(doc_id,lemma) %>% 
          group_by(doc_id) %>% 
          summarise(cleaned_noun = paste(lemma, collapse = " ")) %>% 
          rename(accession_number = doc_id)
        
        # Get the most important POS
        annotated_full <- local_df %>% 
          filter(upos %in% c("ADV","ADJ","NOUN", "AUX", "PART")) %>%
          select(doc_id,lemma) %>% 
          group_by(doc_id) %>% 
          summarise(cleaned_text = paste(lemma, collapse = " ")) %>% 
          rename(accession_number = doc_id)
        
        # Store the data into lists we created before for loop
        local_df <- annotated_nouns %>%
        left_join(annotated_full, by= 'accession_number')
        
   # ----- Insertion to SQL table
  dbWriteTable(conn,"cleaned_10q", local_df, append = TRUE) # create master_index Table
  
  temp_report <- dbGetQuery(conn, paste0('SELECT cik, quarter ,company_name, year_filed, form_type, cleaned_10q.accession_number 
                         FROM master_index LEFT JOIN cleaned_10q ON cleaned_10q.accession_number = master_index.accession_number 
                         WHERE cleaned_10q.accession_number = "',local_df$accession_number[1],'"'))
  
  print(paste(temp_report$form_type, temp_report$quarter, temp_report$year_filed, 'report for CIK:',temp_report$cik, temp_report$company_name, 'has been processed.' ))
      
  rm(annotated_nouns, annotated_full, local_df, temp_report)
  }
z = z + 1
print(paste(z, "out of 499"))

}

```


# Text Cleaning

## TF-IDF Stopwords Identification
```{r}

# ----- Sample on S&P Market 2011 and 2017
conn <- dbConnect(RSQLite::SQLite(), "edgar.db")
sample_reports <- dbGetQuery(conn, 'SELECT cik, cleaned_noun FROM master_index WHERE year_filed IN (2011, 2017)')

# ----- Calculate TF-IDF
tf_idf_samples <- sample_reports %>%
  unnest_tokens(word, cleaned_noun) %>% 
  count(cik, word, sort = TRUE) %>% 
  ungroup() %>%
  bind_tf_idf(word, cik, n)

rm(sample_reports)

summarised_tf_idf <- tf_idf_samples %>%
  group_by(word) %>%
  summarise(avg_tf_idf = mean(tf_idf)) %>%
  arrange(desc(avg_tf_idf))

rm(tf_idf_samples)

# ----- Plotting TF-IDF Distribution
ggplot(summarised_tf_idf, aes(x=avg_tf_idf)) + 
  geom_histogram(color="black", fill="black", bins = 200) + 
  scale_y_log10() +
  labs(title = "TF-IDF Distribution on log-scale")


# ----- Adding buttom 10% words with the lowest TF-IDF as Stopwords
top_90_percent <- summarised_tf_idf %>%
  top_frac(0.90) %>%
  arrange(desc(avg_tf_idf))

buttom_10_percent <- summarised_tf_idf %>%
  anti_join(top_90_percent) %>%
  arrange(desc(avg_tf_idf))

stopw_tfidf <- buttom_10_percent$word
stopw_final <- c(stopw_final, stopw_tfidf)

rm(stopw_tfidf, summarised_tf_idf, top_90_percent, buttom_10_percent)
```

```{r}
dbExecute(conn, 'ALTER TABLE master_index ADD COLUMN cleaned_text TEXT;') # add cleaned_text to master_index
dbExecute(conn, 'ALTER TABLE master_index ADD COLUMN cleaned_noun TEXT;') # add clean noun column to master_index
```

## Retokenise and Remove Stop Words 
```{r}
# ------ 10-K MDA Text Cleaning
conn <- dbConnect(RSQLite::SQLite(), "edgar.db")


market_level <- dbGetQuery(conn, 'SELECT company_name, master_index.accession_number, cleaned_10k_mda.cleaned_text, cleaned_10k_mda.cleaned_noun
           FROM master_index INNER JOIN cleaned_10k_mda ON master_index.accession_number = cleaned_10k_mda.accession_number ')

for(i in 1:nrow(market_level)){
  tryCatch({
    x <- market_level[i,]
    name <- strsplit(tolower(x[1]), " ")[[1]] # identify name of the company
    
    cleaned_noun <- x %>%
      unnest_tokens(word, cleaned_noun) %>%
      filter(!word %in% name) %>% # remove company name
      filter(!word %in% stopw_final) %>% # remove stopwords
      summarise(cleaned_noun = paste(word, collapse = " ")) # combine tokens to store
    
    cleaned_text <- x %>%
      unnest_tokens(word, cleaned_text) %>%
      filter(!word %in% name) %>%
      filter(!word %in% stopw_final) %>%
      summarise(cleaned_text = paste(word, collapse = " "))
    
    accession_number <- x$accession_number[1]
    
    
    dbExecute(conn, paste0("UPDATE master_index SET cleaned_text = '",cleaned_text ,"', cleaned_noun = '",cleaned_noun ,"' WHERE accession_number = '",accession_number ,"'"))
    rm(x, name, cleaned_noun, cleaned_text, accession_number)
    
    print(paste(i, "of", nrow(market_level), "10K MDA"))
    
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
```

```{r}
# ----- 10-Q Text Cleaning
market_level <- dbGetQuery(conn, 'SELECT company_name, master_index.accession_number, cleaned_10q.cleaned_text, cleaned_10q.cleaned_noun
           FROM master_index INNER JOIN cleaned_10q ON master_index.accession_number = cleaned_10q.accession_number ')

for(i in 1:nrow(market_level)){
  tryCatch({
  x <- market_level[i,]
  name <- strsplit(tolower(x[1]), " ")[[1]]
  
  cleaned_noun <- x %>%
    unnest_tokens(word, cleaned_noun) %>%
    filter(!word %in% name) %>%
    filter(!word %in% stopw_final) %>%
    summarise(cleaned_noun = paste(word, collapse = " "))
  
  cleaned_text <- x %>%
    unnest_tokens(word, cleaned_text) %>%
    filter(!word %in% name) %>%
    filter(!word %in% stopw_final) %>%
    summarise(cleaned_text = paste(word, collapse = " "))
  
  accession_number <- x$accession_number[1]
  
  
  dbExecute(conn, paste0("UPDATE master_index SET cleaned_text = '",cleaned_text ,"', cleaned_noun = '",cleaned_noun ,"' WHERE accession_number = '",accession_number ,"'"))
  rm(x, name, cleaned_noun, cleaned_text, accession_number)
  
  print(paste(i, "of", nrow(market_level), "10Q reports"))
  
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}

```



# TF-IDF Analysis
```{r}
# ----- TF-IDF Market Level
market_level <- dbGetQuery(conn, 'SELECT year_filed, cleaned_noun FROM master_index')

market_tokens <- market_level %>% 
  unnest_tokens(word, cleaned_noun) %>% 
  count(year_filed, word, sort = TRUE) %>% 
  ungroup() %>%
  bind_tf_idf(word, year_filed, n)
rm(market_level)

market_tokens %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% group_by(year_filed) %>%
  top_n(20) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = year_filed)) + geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf", title = "Important Terms for S&P 500") +
  facet_wrap(~year_filed, ncol = 4, scales = "free") + coord_flip()

```


```{r}
# ----- TF-IDF Sector Level


# ----- Define financial Era
financial_era <- c('Post Financial Crisis (2009 - 2010)', 'CITIS (2011 - 2012)', 'SICAD (2014 - 2015) ', 'Brexit (2016 - 2019)', 'TCJA (2018 - 2019)') 
years <- c('2009,2010', '2011,2012', '2014,2015', '2016,2017,2018,2019', '2018,2019')

for (era in 1:length(financial_era)) {

  gics_level <- dbGetQuery(conn, paste0('SELECT gics_sector, cleaned_noun FROM sp500 LEFT JOIN master_index ON master_index.cik =  sp500.cik 
                             WHERE year_filed IN (', years[era],')'))
  
  # ----- Calculate TF-IDF by sector on a financial era
  gics_tokens <- gics_level %>% 
    unnest_tokens(word, cleaned_noun) %>% 
    count(gics_sector, word, sort = TRUE) %>% 
    ungroup() %>%
    bind_tf_idf(word, gics_sector, n)
  rm(gics_level)
  
  # ----- Plot the important terms
  gics_tokens %>%
    arrange(desc(tf_idf)) %>%
    mutate(word = factor(word, levels = rev(unique(word)))) %>% group_by(gics_sector) %>%
    top_n(20) %>%
    ungroup %>%
    ggplot(aes(word, tf_idf, fill = gics_sector)) + geom_col(show.legend = FALSE) +
    labs(x = NULL, y = "tf-idf", title = paste("Important Terms", financial_era[era]), subtitle = "grouped by GICS sector") +
    
    facet_wrap(~gics_sector, ncol = 4, scales = "free") + coord_flip()

}
```


